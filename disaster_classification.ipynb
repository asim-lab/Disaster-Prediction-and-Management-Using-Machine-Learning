{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "project-intro",
   "metadata": {},
   "source": [
    "# Disaster Classification using Neural Network and XGBoost\n",
    "\n",
    "This notebook demonstrates a machine learnings that uses deep neural networks for feature extraction and XGBoost for final classification. It focuses on classifying disaster types (Wildfire, Flood, Earthquake) using the EM-DAT dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step-1-intro",
   "metadata": {},
   "source": [
    "## Step 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step-1-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Data Loading, Cleaning, Encoding, Scaling, Balancing\n",
    "# Run this script first\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "file_path = \"E:/Chula University/Disaster Paper/emdat.xlsx\"\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "relevant_disasters = [\"Wildfire\", \"Flood\", \"Earthquake\"]\n",
    "filtered_data = data[data[\"Disaster Type\"].isin(relevant_disasters)]\n",
    "\n",
    "num_features = filtered_data.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
    "cat_features = filtered_data.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "num_imputer = SimpleImputer(strategy=\"median\")\n",
    "filtered_data[num_features] = num_imputer.fit_transform(filtered_data[num_features])\n",
    "cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "filtered_data[cat_features] = cat_imputer.fit_transform(filtered_data[cat_features])\n",
    "\n",
    "label_encoders = {}\n",
    "for col in cat_features:\n",
    "    label_encoders[col] = LabelEncoder()\n",
    "    filtered_data[col] = label_encoders[col].fit_transform(filtered_data[col])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "filtered_data[num_features] = scaler.fit_transform(filtered_data[num_features])\n",
    "\n",
    "X = filtered_data.drop(\"Disaster Type\", axis=1)\n",
    "y = filtered_data[\"Disaster Type\"]\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "import torch\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "\n",
    "# Save for later steps\n",
    "torch.save((X_train_tensor, y_train_tensor, X_test_tensor, X_train, X_test, y_train, y_test, y_resampled), \"processed_data.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step-2-intro",
   "metadata": {},
   "source": [
    "## Step 2: Neural Network Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step-2-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Train Neural Network Feature Extractor\n",
    "# Run after 1_data_preprocessing.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "X_train_tensor, y_train_tensor, X_test_tensor, X_train, X_test, y_train, y_test, y_resampled = torch.load(\"processed_data.pt\")\n",
    "\n",
    "class EnhancedFeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EnhancedFeatureExtractor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.4)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.act2(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "hidden_size = 128\n",
    "feature_extractor = EnhancedFeatureExtractor(input_size=X_train_tensor.shape[1], hidden_size=hidden_size)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "feature_extractor.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(feature_extractor.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "batch_size = 64\n",
    "num_epochs = 50\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Training Epochs\"):\n",
    "    feature_extractor.train()\n",
    "    for batch in train_loader:\n",
    "        X_batch, y_batch = batch\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        embeddings = feature_extractor(X_batch)\n",
    "        outputs = nn.Linear(hidden_size // 2, len(np.unique(y_train_tensor.cpu().numpy()))).to(device)(embeddings)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "feature_extractor.eval()\n",
    "with torch.no_grad():\n",
    "    X_train_embeddings = feature_extractor(X_train_tensor.to(device)).cpu().numpy()\n",
    "    X_test_embeddings = feature_extractor(X_test_tensor.to(device)).cpu().numpy()\n",
    "\n",
    "np.savez(\"embeddings.npz\", X_train=X_train_embeddings, X_test=X_test_embeddings)\n",
    "torch.save(feature_extractor.state_dict(), \"feature_extractor.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step-3-intro",
   "metadata": {},
   "source": [
    "## Step 3: XGBoost Classifier with GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step-3-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Train XGBoost on extracted embeddings\n",
    "# Run after 2_train_feature_extractor.py\n",
    "import numpy as np\n",
    "import torch\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "embeddings = np.load(\"embeddings.npz\")\n",
    "X_train_embeddings, X_test_embeddings = embeddings[\"X_train\"], embeddings[\"X_test\"]\n",
    "_, _, _, _, _, y_train, y_test, y_resampled = torch.load(\"processed_data.pt\")\n",
    "relevant_disasters = [\"Wildfire\", \"Flood\", \"Earthquake\"]\n",
    "\n",
    "param_grid = {\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "    \"max_depth\": [5, 7, 9],\n",
    "    \"n_estimators\": [100, 200, 300]\n",
    "}\n",
    "xgb_model = GridSearchCV(\n",
    "    XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\"),\n",
    "    param_grid,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=3,\n",
    "    verbose=1\n",
    ")\n",
    "xgb_model.fit(X_train_embeddings, y_train)\n",
    "y_pred = xgb_model.predict(X_test_embeddings)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=relevant_disasters))\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=relevant_disasters, yticklabels=relevant_disasters)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
